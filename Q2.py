# -*- coding: utf-8 -*-
"""HW2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uijH3UCS3OEya1LrP2GaxPz6R4FZKEAD
"""

import numpy as np
import scipy.io as sio
import random
from sklearn.model_selection import StratifiedKFold
from sklearn.svm import LinearSVC
from sklearn.datasets import make_classification
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import model_selection
from sklearn.multiclass import OneVsRestClassifier
from numpy import linalg as LA
import time

data = sio.loadmat('q1_dataset.mat', squeeze_me=False, chars_as_strings=False, mat_dtype=True, struct_as_record=True)
data2 = sio.loadmat('q2_dataset.mat', squeeze_me=False, chars_as_strings=False, mat_dtype=True, struct_as_record=True)

hog_features_test = data['hog_features_test']
hog_features_train = data['hog_features_train']
hogfsize = hog_features_train.shape[1]
inception_features_train = data['inception_features_train']
inception_features_test = data['inception_features_test']
superclass_labels_train = data['superclass_labels_train']
superclass_labels_test = data['superclass_labels_test']
subclass_labels_train = data['subclass_labels_train']
subclass_labels_test = data['subclass_labels_test']

def calculations(pred, actual):
  confusionmtarix = np.zeros((2,2))
  TP = np.sum(actual[pred[:]==0,0]==0)
  TN = np.sum(actual[pred[:]==1,0]==1)
  FP = np.sum(actual[pred[:]==1,0]==0)
  FN = np.sum(actual[pred[:]==0,0]==1)
  confusionmtarix[0,0] = TP
  confusionmtarix[1,1] = TN
  confusionmtarix[0,1] = FP
  confusionmtarix[1,0] = FN
  Prec = TP/(TP+FP)
  Rec = TP/(TP+FN)
  NPV = TN/(TN+FN)
  FDR = FP/(FP+TP)
  F1 = (2*Prec*Rec)/(Prec+Rec)
  accuracy = (np.sum(pred == actual[:,0])/actual.shape[0])*100
  print('Precision: ', Prec)
  print('Recall: ', Rec)
  print('FDR: ', FDR)
  print('F1: ', F1)
  print('NPV: ', NPV)
  print('Accuracy: ', accuracy)
  print('Confusion Matrix')
  print(confusionmtarix)
  return accuracy, Prec, Rec, F1

def training(traindata, batchsize, learningR, trainlabel):
  datacol = traindata.shape[1]
  batch_size = batchsize
  w = np.random.normal(scale = 0.01 , size = datacol+1)
  learnR = learningR
  py1 = np.zeros(traindata.shape[0])

  for iterate in range(1000):
    for s in range(int(traindata.shape[0]/batchsize)):
      i = random.randint(0, (traindata.shape[0]/batchsize)-1)
      if batchsize ==2000:
        i=s
      start = i*batch_size
      stop = (i+1)*batch_size
      exps = np.exp(w[0]+np.sum(w[1:datacol+1]*traindata[start:stop, :], axis = 1))
      py1[start:stop] = exps/(1 + exps)
      lblpy1 = np.zeros(batchsize)
      lblpy1 = ((trainlabel)[start:stop,0])-py1[start:stop]
      w[0] = w[0] + (learnR*(np.sum(lblpy1)))/batchsize
      lblpy1 = np.reshape(lblpy1, (-1,batchsize))
      lbls2 = np.transpose(np.repeat(lblpy1, datacol, axis =0))
      w[1:] = w[1:] + (learnR*np.reshape(np.sum(traindata[start:stop,:]*lbls2 , axis = 0), (-1,datacol)))/batchsize
  return w

def testing(weights, testdata, testlabel):
  lbls = np.zeros(testdata.shape[1]+1)
  lbls = (weights[0]+np.sum(weights[1:]*testdata,axis=1))>0
  confusionmtarix = np.zeros((2,2))
  TP = np.sum(testlabel[lbls[:]==0,0]==0)
  TN = np.sum(testlabel[lbls[:]==1,0]==1)
  FP = np.sum(testlabel[lbls[:]==1,0]==0)
  FN = np.sum(testlabel[lbls[:]==0,0]==1)
  confusionmtarix[0,0] = TP
  confusionmtarix[1,1] = TN
  confusionmtarix[0,1] = FP
  confusionmtarix[1,0] = FN
  Prec = TP/(TP+FP)
  Rec = TP/(TP+FN)
  NPV = TN/(TN+FN)
  FDR = FP/(FP+TP)
  FPR = FP/(TN+FP)
  F1 = (2*Prec*Rec)/(Prec+Rec)
  F2 = (5*Prec*Rec)/(4*Prec +Rec)
  accuracy = (np.sum(lbls == testlabel[:,0])/testdata.shape[0])*100
  print('Precision: ', Prec)
  print('Recall: ', Rec)
  print('FDR: ', FDR)
  print('FPR: ', FPR)
  print('F1: ', F1)
  print('F2: ', F2)
  print('NPV: ', NPV )
  print('Accuracy: ', accuracy)
  print('Confusion Matrix:   ') 
  print(confusionmtarix)
  return accuracy

#Batch gradient for hog features
print('25 Batch for hog features')
t1 = time.time()
hogbatchw = training(hog_features_train, 25, 0.001, superclass_labels_train)
hogbatch_result =  testing(hogbatchw, hog_features_test, superclass_labels_test)
t0 = time.time()
print('Time: ', t0-t1)

#Batch gradient descent NN features
print('--------------------')
print('25 batch for inception feautres')
t1 = time.time()
incep_batch_w = training(inception_features_train, 25, 0.001, superclass_labels_train)
incepbatch_result = testing(incep_batch_w, inception_features_test, superclass_labels_test)
t0 = time.time()
print('Time: ', t0-t1)

print('------------')
print('Stochiastic for hog features')
t1 = time.time()
coff2 = training(hog_features_train, 1, 0.001, superclass_labels_train)
hog_stoc_result = testing(coff2, hog_features_test, superclass_labels_test)
t0 = time.time()
print('Time: ', t0-t1)

print('-------------')
print('Stochiastic for inception features')
t1 = time.time()
coff3 = training(inception_features_train, 1, 0.001, superclass_labels_train)
incep_stoc_result = testing(coff3, inception_features_test, superclass_labels_test)
t0 = time.time()
print('Time: ', t0-t1)

print('--------------')
print('Full batch for inception')
t1 = time.time()
coff4 = training(inception_features_train, 2000, 0.001, superclass_labels_train)
full_hogres = testing(coff4, inception_features_test, superclass_labels_test)
t0 = time.time()
print('Time: ', t0-t1)
print('The index of top 10 important features are ', coff4.argsort()[-10:][::-1])

print('--------------')
print('Full batch for hog')
t1 = time.time()
coff5 = training(hog_features_train, 2000, 0.001, superclass_labels_train)
fullincep_res = testing(coff5, hog_features_test, superclass_labels_test)
t0 = time.time()
print('Time: ', t0-t1)
print('The index of top 10 important features are ', coff5.argsort()[-10:][::-1])

def linearSVM(traindata, trainlabel, testdata, testlabel, kfol, svmtype):
  splt = model_selection.StratifiedKFold(n_splits=kfol, shuffle=True, random_state=5)
  meanacc = np.zeros(5)
  for val in range(-2,3):
    ix = 0 
    accu = np.zeros(5)
    for train_index, test_index in splt.split(traindata, trainlabel):
      X_train, X_test = traindata[train_index], traindata[test_index]
      y_train, y_test = trainlabel[train_index], trainlabel[test_index]
      svclassifier = SVC(kernel=svmtype, C = 10**(val))
      svclassifier.fit(X_train, y_train[:,0])
      y_pred = svclassifier.predict(X_test)
      accu[ix] = (np.sum(y_pred[:]== y_test[:,0] ))/y_test.shape[0]
      
      ix+=1
    meanacc[val+2] = np.mean(accu)
    print('Accuracy for C = ', 10**(val) , ' is ', meanacc[val+2]*100 )
  Cs = 10**(np.where(meanacc == np.max(meanacc))[0].tolist()[0]-2)
  svclassifierf = SVC(kernel=svmtype, C = Cs)
  svclassifierf.fit(traindata, trainlabel[:,0])
  y_predf = svclassifierf.predict(testdata)
  accuracyf = (np.sum(y_predf[:]== testlabel[:,0] ))/testlabel.shape[0]
  calculations(y_predf,testlabel)   
  return accuracyf,Cs

print('---------------')
print('Linear SVM for hog features')
t1 = time.time()
hoglinsvm = linearSVM(hog_features_train, superclass_labels_train, hog_features_test, superclass_labels_test,5, 'linear')
print('Best C', hoglinsvm[1])
t0 = time.time()
print('Time: ', t0-t1)

print('---------------')
print('Linear SVM for inception features')
t1 = time.time()
inceplinSVM = linearSVM(inception_features_train, superclass_labels_train, inception_features_test, superclass_labels_test,5, 'linear')
print('Best C', inceplinSVM[1])
t0 = time.time()
print('Time: ', t0-t1)

def hardrbfSVM(traindata, trainlabel, testdata, testlabel, kfol, svmtype):
  splt = model_selection.StratifiedKFold(n_splits=kfol, shuffle=True, random_state=5)
  gamma = np.array([2**(-4), 2**(-3), 2**(-2), 2**(-1), 2**(0), 2**(1), 2**(6)])
  meanacc = np.zeros(len(gamma))
  for val in range(len(gamma)):
    ix = 0 
    accu = np.zeros(5)
    for train_index, test_index in splt.split(traindata, trainlabel):
      X_train, X_test = traindata[train_index], traindata[test_index]
      y_train, y_test = trainlabel[train_index], trainlabel[test_index]
      svclassifier = SVC(kernel=svmtype, gamma = gamma[val], C =1000)
      svclassifier.fit(X_train, y_train[:,0])
      y_pred = svclassifier.predict(X_test)
      accu[ix] = (np.sum(y_pred[:]== y_test[:,0] ))/y_test.shape[0]
      ix+=1
    meanacc[val] = np.mean(accu)
    print('Accuracy for gamma = ', gamma[val] , ' is ', meanacc[val]*100 )
  gammaS = gamma[np.where(meanacc == np.max(meanacc))[0].tolist()[0]]
  svclassifierf = SVC(kernel=svmtype, gamma = gammaS)
  svclassifierf.fit(traindata, trainlabel[:,0])
  y_predf = svclassifierf.predict(testdata)
  accuracyf = (np.sum(y_predf[:]== testlabel[:,0] ))/testlabel.shape[0] 
  calculations(y_predf, testlabel)  
  return accuracyf,gammaS

print('-------------')
print('Hard margin rbf SVM for hog')
t1 = time.time()
hogrbfsvm = hardrbfSVM(hog_features_train, superclass_labels_train, hog_features_test, superclass_labels_test,5, 'rbf')
t0 = time.time()
print('Best gamma ', hogrbfsvm[1])
print('Time: ', t0-t1)

print('-------------')
print('Hard margin rbf SVM for inception')
t1 = time.time()
inceprbfsvm = hardrbfSVM(inception_features_train, superclass_labels_train, inception_features_test, superclass_labels_test,5, 'rbf')
t0 = time.time()
print('Best gamma ', inceprbfsvm[1])
print('Time: ', t0-t1)

def softrbfSVM(traindata, trainlabel, testdata, testlabel, kfol, Ca, svmtype):
  cd = Ca
  splt = model_selection.StratifiedKFold(n_splits=kfol, shuffle=True, random_state=5)
  gammaa = np.array([2**(-2), 2**(1), 2**(6)])
  meanacc = np.zeros(len(gammaa))
  for val in range(len(gammaa)):
    ix = 0 
    accu = np.zeros(5)
    for train_index, test_index in splt.split(traindata, trainlabel):
      X_train, X_test = traindata[train_index], traindata[test_index]
      y_train, y_test = trainlabel[train_index], trainlabel[test_index]
      svclassifier = SVC(kernel=svmtype, gamma = gammaa[val], C=Ca)
      svclassifier.fit(X_train, y_train[:,0])
      y_pred = svclassifier.predict(X_test)
      accu[ix] = (np.sum(y_pred[:]== y_test[:,0] ))/y_test.shape[0]
      ix+=1
    meanacc[val] = np.mean(accu)
    print('Accuracy for gamma = ', gammaa[val] ,'and C = ',cd, ' is ', meanacc[val]*100 )
  gammaS = gammaa[np.where(meanacc == np.max(meanacc))[0].tolist()[0]]
  return gammaS, np.max(meanacc)

def softSvmtraining(traindata, trainlabel, testdata, testlabel, kfold, svmtype):
  C = np.array([10**(-2), 10**(1), 10**(2)])
  higheacc = np.zeros(3)
  maxgamas = np.zeros(3)
  maxC = np.zeros(3)
  for val2 in range(len(C)):
    s = softrbfSVM(traindata, trainlabel, testdata, testlabel,kfold,C[val2], svmtype)
    higheacc[val2] = s[1]
    maxgamas[val2] = s[0]
    maxC[val2] = C[val2]
  mC = maxC[np.where(higheacc == np.max(higheacc))[0].tolist()[0]]
  mG = maxgamas[np.where(higheacc == np.max(higheacc))[0].tolist()[0]]
  mA =  np.max(higheacc)
  print('The highest accuracy in 5 kfold',mA*100, 'with C ', mC,'with gamma',mG )
  svclassifierf = SVC(kernel='rbf', gamma = mG, C = mC)
  svclassifierf.fit(traindata, trainlabel[:,0])
  y_predf = svclassifierf.predict(testdata)
  print('Values for the whole data: ')
  calculations(y_predf, testlabel)
  accuracyf = (np.sum(y_predf[:]== testlabel[:,0] ))/testlabel.shape[0]   
  return accuracyf, mG, mC, mA

print('-------------')
print('Soft margin with gamma and C rbf SVM for hog')
t1 = time.time()
hogsoftrbf = softSvmtraining(hog_features_train, superclass_labels_train, hog_features_test,superclass_labels_test, 5, 'rbf')
t0 = time.time()
print('Best gamma ', hogsoftrbf[1])
print('Best C ', hogsoftrbf[2])
print('Time: ', t0-t1)



print('-------------')
print('Soft margin with gamma and C rbf SVM for inception')
t1 = time.time()
incepsoftrbf = softSvmtraining(inception_features_train, superclass_labels_train, inception_features_test,superclass_labels_test, 5, 'rbf')
t0 = time.time()
print('Best gamma ', hogsoftrbf[1])
print('Best C ', hogsoftrbf[2])
print('Time: ', t0-t1)

def softOnevsALLSVM(traindata, trainlabel, testdata, testlabel, kfol, Ca,svmtype):
  splt = model_selection.StratifiedKFold(n_splits=kfol, shuffle=True, random_state=5)
  cde =Ca
  gammaa = np.array([2**(-2), 2**(1), 2**(6)])
  meanacc = np.zeros(len(gammaa))
  for val in range(len(gammaa)):
    ix = 0 
    accu = np.zeros(5)
    for train_index, test_index in splt.split(traindata, trainlabel):
      X_train, X_test = traindata[train_index], traindata[test_index]
      y_train, y_test = trainlabel[train_index], trainlabel[test_index]
      svclassifier = OneVsRestClassifier(SVC(kernel=svmtype, gamma = gammaa[val], C=Ca))
      svclassifier.fit(X_train, y_train[:,0])
      y_pred = svclassifier.predict(X_test)
      accu[ix] = (np.sum(y_pred[:]== y_test[:,0] ))/y_test.shape[0]
      ix+=1
    meanacc[val] = np.mean(accu)
    print('Accuracy for gamma ', gammaa[val], 'and C ', cde, 'is ', meanacc[val] )
  gammaS = gammaa[np.where(meanacc == np.max(meanacc))[0].tolist()[0]]
  return gammaS, np.max(meanacc)

def softonevsALLSvmtraining(traindata, trainlabel, testdata, testlabel, kfold, svmtype):
  C = np.array([10**(-2), 1, 10**(2)])
  higheacc = np.zeros(3)
  maxgamas = np.zeros(3)
  maxC = np.zeros(3)
  for val2 in range(len(C)):
    s = softOnevsALLSVM(traindata, trainlabel, testdata, testlabel,kfold,C[val2], svmtype)
    higheacc[val2] = s[1]
    maxgamas[val2] = s[0]
    maxC[val2] = C[val2]
  mC = maxC[np.where(higheacc == np.max(higheacc))[0].tolist()[0]]
  mG = maxgamas[np.where(higheacc == np.max(higheacc))[0].tolist()[0]]
  mA =  np.max(higheacc)
  print('The highest accuracy in 5 kfold is ',mA*100, 'with C ', mC,'with gamma',mG )
  svclassifierf = OneVsRestClassifier(SVC(kernel='rbf', gamma = mG, C = mC))
  svclassifierf.fit(traindata, trainlabel[:,0])
  y_predf = svclassifierf.predict(testdata)
  print(confusion_matrix(testlabel[:,0], y_predf))
  accuracyf = (np.sum(y_predf[:]== testlabel[:,0] ))/testlabel.shape[0]   
  return accuracyf*100, mG, mC, mA

print('-------------')
print('Soft margin One vs All with gamma and C rbf SVM for hog')
t1 = time.time()
hogOvAsoftrbf = softonevsALLSvmtraining(hog_features_train, subclass_labels_train, hog_features_test,subclass_labels_test, 5, 'rbf')
t0 = time.time()
print('Best gamma ', hogOvAsoftrbf[1])
print('Best C ', hogOvAsoftrbf[2])
print('Accuracy: ', hogOvAsoftrbf[0] )
print('Time: ', t0-t1)

print('-------------')
print('Soft margin One vs All with gamma and C rbf SVM for incep')
t1 = time.time()
incepOvAsoftrbf = softonevsALLSvmtraining(inception_features_train, subclass_labels_train, inception_features_test,subclass_labels_test, 5, 'rbf')
t0 = time.time()
print('Best gamma ', incepOvAsoftrbf[1])
print('Best C ', incepOvAsoftrbf[2])
print('Accuracy: ', incepOvAsoftrbf[0] )
print('Time: ', t0-t1)

def hardpolynomSVM(traindata, trainlabel, testdata, testlabel, kfol, d,svmtype):
  splt = model_selection.StratifiedKFold(n_splits=kfol, shuffle=True, random_state=5)
  gammaa = np.array([2**(-2), 2**(1), 2**(6)])
  ds =d
  meanacc = np.zeros(len(gammaa))
  for val in range(len(gammaa)):
    ix = 0 
    accu = np.zeros(5)
    for train_index, test_index in splt.split(traindata, trainlabel):
      X_train, X_test = traindata[train_index], traindata[test_index]
      y_train, y_test = trainlabel[train_index], trainlabel[test_index]
      svclassifier = OneVsRestClassifier(SVC(kernel=svmtype, gamma = gammaa[val], degree=d,C=10000))
      svclassifier.fit(X_train, y_train[:,0])
      y_pred = svclassifier.predict(X_test)
      accu[ix] = (np.sum(y_pred[:]== y_test[:,0] ))/y_test.shape[0]
      ix+=1
    meanacc[val] = np.mean(accu)
    print('Accuracy for gamma', gammaa[val],'and d ', ds, 'is ', meanacc[val]*100)
  gammaS = gammaa[np.where(meanacc == np.max(meanacc))[0].tolist()[0]]
  return gammaS, np.max(meanacc)

def hardPolSvmtraining(traindata, trainlabel, testdata, testlabel, kfold, svmtype):
  d = np.array([3, 5, 7])
  higheacc = np.zeros(3)
  maxgamas = np.zeros(3)
  maxd = np.zeros(3)
  for val2 in range(len(d)):
    s = hardpolynomSVM(traindata, trainlabel, testdata, testlabel,kfold,d[val2], svmtype)
    higheacc[val2] = s[1]
    maxgamas[val2] = s[0]
    maxd[val2] = d[val2]
  md = maxd[np.where(higheacc == np.max(higheacc))[0].tolist()[0]]
  mG = maxgamas[np.where(higheacc == np.max(higheacc))[0].tolist()[0]]
  mA =  np.max(higheacc)
  print('the highest accuracy in 5 kfold',mA*100, 'with d ', md,'with gamma',mG )
  svclassifierf = OneVsRestClassifier(SVC(kernel=svmtype, gamma = mG, degree = md, C=10000000))
  svclassifierf.fit(traindata, trainlabel[:,0])
  y_predf = svclassifierf.predict(testdata)
  print(confusion_matrix(testlabel[:,0], y_predf))
  accuracyf = (np.sum(y_predf[:]== testlabel[:,0] ))/testlabel.shape[0]   
  return accuracyf*100, mG, md, mA

print('-------------')
print('Hardmargin One vs All with gamma and C rbf SVM for hog')
t1 = time.time()
hogpoltrain = hardPolSvmtraining(hog_features_train, subclass_labels_train, hog_features_test, subclass_labels_test, 5, 'poly')
t0 = time.time()
print('Best d ', hogpoltrain[2])
print('Best gamma ', hogpoltrain[1])
print('Accuracy: ', hogpoltrain[0])
print('Time: ', t0-t1)

print('-------------')
print('Hardmargin One vs All with gamma and C rbf SVM for incep')
t1 = time.time()
inceppoltrain = hardPolSvmtraining(inception_features_train, subclass_labels_train, inception_features_test, subclass_labels_test, 5, 'poly')
t0 = time.time()
print('Best d ', inceppoltrain[2])
print('Best gamma ', inceppoltrain[1])
print('Accuracy: ', inceppoltrain[0])
print('Time: ', t0-t1)

#SVD based implementation

X = data2['data']
flatX = np.transpose(X.reshape(-1, X.shape[0]))
t1 = time.time()
U, sigma, Vt = LA.svd(flatX, full_matrices=False, compute_uv=True)
reconstructed_data = np.dot((np.dot(U, np.diag(sigma))), Vt)
mse = (np.square(reconstructed_data - flatX)).mean(axis=None)
t0 = time.time()
print('Time is ', t0-t1, 'and MSE is ', mse)

t3 = time.time()
x_ = np.mean(flatX[:,:], axis =0)
x = (flatX[:,:] - x_)
covx =(np.transpose(x).dot(x))/(flatX.shape[0]-1)
eigval, eigvec = LA.eig(covx)
eigval_real = np.real(eigval)
eigvec_real = np.real(eigvec)
reconstructed_data2 = (flatX[:,:]).dot(eigvec_real)
mse2 = (np.square(reconstructed_data2 - flatX)).mean(axis=None)
t4 = time.time()
print('Time is ', t4-t3, 'and MSE is ', mse2)

import matplotlib.pyplot as plt

f, axarr = plt.subplots(3,5)
for i in range(5):
  axarr[2,i].imshow(reconstructed_data2.T.reshape(X.shape[0], X.shape[1], X.shape[2])[i])
  # plt.show()
  axarr[1,i].imshow(reconstructed_data.T.reshape(X.shape[0], X.shape[1], X.shape[2])[i])
  # plt.show()
  axarr[0,i].imshow(X[i,:,:])

